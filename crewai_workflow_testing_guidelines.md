# CrewAI Workflow & FastAPI Endpoint: Conceptual Testing Guidelines
## (Example: `trading_crew` and `/crew/run_trading_analysis` endpoint)

## Introduction

This document provides a conceptual framework and general guidelines for testing CrewAI workflows, their triggering mechanisms (such as FastAPI endpoints), and associated components like trade executors and task logging. The "trading_crew" is used as a primary example. Actual test implementation will depend on the specific architecture and details of the project.

## 1. Prerequisites for Testing

Effective testing of a CrewAI workflow and its ecosystem requires:

*   **Access to CrewAI Definitions:** The Python code defining the `trading_crew`, including its:
    *   Agents (e.g., `MarketAnalystAgent`, `StrategyAgent`, `TradeAdvisorAgent`) and their configurations (LLMs, tools, backstories).
    *   Tasks assigned to each agent, including their descriptions, expected outputs, and inter-dependencies.
    *   The `Crew` object itself, detailing the workflow process (e.g., sequential, hierarchical).
*   **Access to FastAPI Endpoint Code:** The code for the endpoint that triggers the crew, for example, `/crew/run_trading_analysis` in `python-ai-services/main.py`.
*   **Access to Related Components:**
    *   `SimulatedTradeExecutor`: The component responsible for processing the `TradeSignal` generated by the crew for paper trading.
    *   `AgentTask` Logging: The mechanism or service responsible for creating and updating task records (e.g., in a database) when a crew is run.
*   **Understanding of Expected Output:** Clear definition of the final output structure from the crew, such as a `TradeSignal` Pydantic model.
*   **Knowledge of AG-UI Event Emission:** Understanding how and when AG-UI (Agent-to-User Interface) events are emitted during the crew's execution (e.g., task status changes, final signal generation) and how these might be broadcast (e.g., via `A2AProtocol` or a dedicated WebSocket service).

## 2. Testing the FastAPI Endpoint (`/crew/run_trading_analysis`)

This involves testing the endpoint that initiates the CrewAI workflow.

### Integration Test Approach:
*   Utilize FastAPI's `TestClient` for synchronous testing of async endpoints, or an async HTTP client like `httpx` if preferred for async test suites.

### Request Validation:
*   **Valid Input:** Send a valid JSON payload to the endpoint and verify a successful HTTP response (e.g., 200 OK or 202 Accepted if the crew execution is handled as a background task).
*   **Invalid Input:** Test with various forms of invalid input:
    *   Missing required fields in the payload.
    *   Incorrect data types for fields.
    *   Malformed JSON.
    *   Assert that the endpoint returns appropriate HTTP error codes (e.g., 400 Bad Request, 422 Unprocessable Entity) and informative error messages.

### Successful Crew Execution (Endpoint Perspective):
*   **Mock External Dependencies:**
    *   If the crew relies on external services that are not part of the system under test (e.g., third-party market data APIs not proxied through `MarketDataService`, or direct calls to paid LLM services that you want to avoid in tests), these should be mocked.
    *   LLM calls within the crew should ideally be mocked at this stage to ensure test speed and determinism (see Section 3).
*   **Trigger and Assert Response:**
    *   Make a request to the endpoint with a valid payload.
    *   Assert that the HTTP response status code is as expected (e.g., 200 OK if the result is synchronous, or 202 Accepted if it kicks off a background task).
*   **Verify `AgentTask` Logging:**
    *   If the endpoint is responsible for creating or updating an `AgentTask` record (e.g., in a database to track the crew's execution):
        *   Mock the service or database functions responsible for `AgentTask` persistence.
        *   Verify that these functions are called with the correct parameters (e.g., task input, initial status "running", associated user/agent ID).
        *   If the crew execution is synchronous, verify the `AgentTask` is updated to a "completed" status with the result.

### Error Handling:
*   **Crew Instantiation Failure:** Simulate scenarios where the `trading_crew` might fail to instantiate (e.g., due to misconfiguration or issues with mocked dependencies) and ensure the endpoint returns an appropriate HTTP error (e.g., 500 Internal Server Error).
*   **Initial Task Execution Failure:** If the first task in the crew fails immediately, test that the endpoint handles this gracefully and returns a relevant error status.

## 3. Testing the `trading_crew` Logic (CrewAI Unit/Integration Tests)

These tests focus on the CrewAI definition itself, ensuring agents and tasks interact as expected.

### Mocking LLMs:
*   This is crucial for reliable and fast tests. Mock the LLM instances (e.g., `ChatOpenAI`, `AzureChatOpenAI`, or custom LLM wrappers) used by each agent in the crew.
*   The mock should allow you to predefine the responses the LLM will return when an agent calls it. This gives you control over the agent's behavior and output for testing purposes.
    *   Example: `mock_llm.invoke = AsyncMock(return_value="Mocked LLM response for market analysis.")`

### Task Execution & Flow:
*   **Task Order:** For sequential crews, verify that tasks are executed in the defined order. This can be done by checking the order of calls to mocked agent methods or by inspecting logs/outputs generated by each task.
*   **Data/Context Passing:** Ensure that information and context are correctly passed from one task to the next, or from one agent to another, as defined in the task dependencies and output/context settings. Mock the `execute` method of tasks or agent methods to inspect the `context` they receive.

### Agent Output Validation:
*   **Individual Agent Tests:** For each agent in the crew, given a controlled input (passed via a task's context) and a predefined mocked LLM response, test that the agent's processing logic produces the expected output format and content.
*   **Final Agent Output (`TradeSignal`):** Pay special attention to the final agent in the workflow (e.g., `TradeAdvisorAgent`). Verify that its output (which becomes the crew's final result) conforms to the expected `TradeSignal` structure (e.g., using Pydantic model validation if `TradeSignal` is a Pydantic model).

### Tool Usage:
*   If agents utilize tools (other than the default LLM call), these tools should be mocked during tests.
*   Verify that agents call their tools with the correct arguments based on the task input and preceding agent outputs.
    *   Example: If `MarketAnalystAgent` has a tool to fetch specific data using `MarketDataService`, mock this tool and check the parameters passed to it.

### AG-UI Event Emission (from within Crew):
*   If AG-UI events are intended to be emitted directly from within agent methods or task execution callbacks (e.g., to signal "Agent X started task Y", "Task Z completed"):
    *   Mock the mechanism used for event emission (e.g., a call to `A2AProtocol.broadcast_message`, a direct push to a WebSocket service, or a logging call that's monitored for events).
    *   Verify that these event emission calls are made at the appropriate points in the workflow.
    *   Check that the emitted events have the correct `event_type` and that their `payload` contains relevant information (e.g., agent name, task ID, status, intermediate results).

## 4. Testing `SimulatedTradeExecutor` Integration

This ensures that the output of the `trading_crew` is correctly processed for paper trading.

*   **Scenario:** This test runs after a (potentially mocked) successful execution of the `trading_crew` that produces a `TradeSignal`.
*   **Mock `SimulatedTradeExecutor`:** Mock the `SimulatedTradeExecutor` class or its relevant methods (e.g., `execute_paper_trade`, `update_agent_wallet`).
*   **Verify Invocation:** Ensure that the `SimulatedTradeExecutor` is instantiated and its execution method is called with the `TradeSignal` object generated by the crew.
*   **Verify Logging/State Changes:**
    *   If the executor logs paper trades to a database or file, mock the database/file writing functions and assert they are called with the correct trade details.
    *   If it updates an agent's virtual wallet, mock the wallet update mechanism and verify it's called appropriately.

## 5. End-to-End Considerations (Conceptual)

An ideal end-to-end test would provide the highest confidence but is also the most complex to set up and maintain.

*   **Test Flow:**
    1.  A test client makes an HTTP request to the `/crew/run_trading_analysis` endpoint.
    2.  (Optional) A WebSocket test client connects to the AG-UI event stream.
    3.  The test orchestrator observes:
        *   The `AgentTask` record in the database is created and updated (e.g., from "pending" to "running" to "completed").
        *   The WebSocket test client receives the expected sequence of AG-UI events related to the crew's progress and final output.
        *   The `SimulatedTradeExecutor` correctly processes the `TradeSignal` and logs the paper trade.
        *   The HTTP response from the endpoint is correct.
*   **Challenges:** Requires a fully integrated environment. External dependencies (like LLMs) would ideally still be mocked or replaced with fakes to avoid flakiness and cost. Managing state and ensuring idempotency can be difficult.

## 6. Tools for Testing

*   **`pytest`:** A powerful and flexible testing framework for Python (use with `pytest-asyncio` for async code).
*   **FastAPI `TestClient`:** For making requests to the FastAPI application in tests.
*   **`unittest.mock` (or `pytest-mock`):** Essential for creating mocks of LLMs, tools, external services, and internal dependencies.
*   **WebSocket Client Libraries (e.g., `websockets`):** For creating test clients to verify AG-UI event streaming if testing end-to-end event reception.
*   **Pydantic (if used for models):** Useful for validating data structures like `TradeSignal` and AG-UI event payloads even within tests.

## Disclaimer

These are conceptual guidelines. The specific testing strategies and implementation details will need to be adapted based on the unique architecture, technologies, and requirements of the CrewAI application and its surrounding services.
